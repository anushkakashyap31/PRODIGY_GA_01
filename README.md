# ğŸš€ PRODIGY\_GA\_01 â€” GPT-2 Fine-Tuning Project

This repository contains **Task 1: GPT-2 Fine-Tuning Project** as part of my Generative AI Internship at Prodigy Infotech, where I have fine-tuned the GPT-2 model on custom data.

## ğŸ’¡ Project Overview

In this task, I have:

- Fine-tuned the GPT-2 model using Hugging Face Transformers and PyTorch.
- Used a custom text dataset (`mydata.txt`) for training.
- Generated text samples using the fine-tuned model.
The goal is to generate more personalized and meaningful text outputs based on specific data.

## ğŸ“„ Files

- `finetune_gpt2.py`: Script to fine-tune the GPT-2 model on custom data.
- `generate_text.py`: Script to generate text from the fine-tuned GPT-2 model.
- `mydata.txt`: Custom text dataset used for fine-tuning.
- `.gitignore`: To ignore unnecessary files in version control.
- `cached_lm_GPT2Tokenizer_128_mydata.txt`: Tokenizer cache file.

## âš™ï¸ Technologies Used

- Python
- PyTorch
- Hugging Face Transformers
- Accelerate

## ğŸ’¬ How to Run

1ï¸âƒ£ Clone the repository:
```
git clone https://github.com/anushkakashyap31/PRODIGY_GA_01.git
cd PRODIGY_GA_01
```

2ï¸âƒ£ Install dependencies (inside virtual environment):
```
pip install -r requirements.txt
```

3ï¸âƒ£ Run fine-tuning:
```
python finetune_gpt2.py
```

4ï¸âƒ£ Generate text:
```
python generate_text.py
```

## ğŸ¯ Objectives

- Learn the process of fine-tuning large language models.
- Understand text generation using custom-trained models.
- Improve practical skills with Hugging Face Transformers and PyTorch.

## ğŸ‘©â€ğŸ’» Author

- **Anushka Kashyap**

## â­ Acknowledgements

- Prodigy Infotech internship guidance
- Hugging Face Transformers library

âœ¨ Feel free to â­ star this repository if you find it helpful!
